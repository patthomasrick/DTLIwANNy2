import xml.etree.cElementTree as ETree
import pickle
from random import choice

import numpy as np

#  ~/.pyenv/versions/3.5.2/bin/python

"""
xml2trainingdata.py: This module is to convert the .xml file created by ruler.py
to a format understandable by the artificial neural network built by the Fast
Artificial Neural Network library.

The XML file is from data generated by ruler.py.
"""

__author__ = "Patrick Thomas"
__credits__ = ["Patrick Thomas", "Rick Fisher"]
__version__ = "1.0.0"
__date__ = "10/19/16"
__maintainer__ = "Patrick Thomas"
__email__ = "pthomas@mail.swvgs.us"
__status__ = "Development"

# global vars
MIN_LEAVES = 7      # minimum number a leaves to consider a species valid for use with the ANN
NUM_RUN_LEAVES = 3  # number of leaves to remove from a set of training leaves to constitute the run leaves

DEFAULT_XML = 'save-data/leaf-data.xml'
DEFAULT_TRAINING_FILE = 'ann/train.data'
DEFAULT_PICKLE_FILE = 'ann/train-header.pickle'

PCT_TRAIN_DATA = 0.80  # maximum ratio of leaves in the training data to all leave

DEFAULT_SAMPLE_SAVE_FILE = 'ann/ann-sampling.npz'


def convert(xml_file_path, train_data_path):
    """
    Open and parse an .xml file with all leaf data and then save all of
    that data under self.train_file_path in a format the ANN will understand.
    :return: none
    """
    leaf_data = load_xml(xml_file_path)
    save_train(train_data_path, leaf_data, num_species='MAX', min_leaves=MIN_LEAVES)


def load_xml(xml_path):
    """
    Load the .xml file into the memory
    :return: leaf_data
    """

    # initialize variables
    data_dict = {}

    # load XML file
    tree = ETree.parse(xml_path)

    # get the root
    data_root = tree.getroot()

    # iterate through all species in the XML file
    for species in data_root:
        # get the genus and species
        g = species.find('g').text
        s = species.find('s').text

        # append information about all leaves in a genus and species to a list
        species_list = []
        for img in species.findall('leaf'):
            attribs = {
                # not included in data dict
                'img_name': img.get('name'),

                # data that isn't very important to ANN
                'path': img.find('path').text,
                'v_cm': float(img.find('v_cm').text),
                'h_cm': float(img.find('h_cm').text),
                'otsu': float(img.find('otsu').text),

                # simple measurements
                'p': float(img.find('p').text),
                'length': float(img.find('length').text),
                'area': float(img.find('area').text),
                'sf_v_mean': float(img.find('sf_v_mean').text),
                'sf_v_median': float(img.find('sf_v_median').text),

                # vein measurements
                'vein_angle_above': float(img.find('vein_angle_above').text),
                'vein_angle_below': float(img.find('vein_angle_below').text),
                'vein_length': float(img.find('vein_length').text),

                # save location
                'array_files': img.find('array_files').text,
            }

            # # load contour info
            # contour_size = None
            # contour_pos = None
            # contour_angle = None

            with np.load(attribs['array_files']) as array_data:
                contour_size = array_data['contour_size'].tolist()
                contour_pos = array_data['contour_pos'].tolist()
                contour_angle = array_data['contour_angles'].tolist()

            # move contour data to attribs
            attribs['contour_size'] = contour_size
            attribs['contour_pos'] = contour_pos
            attribs['contour_angle'] = contour_angle

            # append the newly found datat to the list
            species_list.append(attribs)

        # save the list to a dictionary with the key being the species' name
        data_dict['{0} {1}'.format(g, s)] = species_list

    # return the data
    return data_dict


def save_train(fname, leaf_data_dict, num_species, min_leaves=10):
    """
    Saves the data dictionary returned by load_xml() to a file readable by FANN.

    format of FANN training data:
    (num of training pairs) (number of inputs) (num of outputs)
    (inputs) (...)
    (outputs) (...)

    :param fname: str, the name of the file that will be saved
    :param leaf_data_dict: the dictionary returned by load_xml()
    :param min_leaves: the minimum number of leaf images allowed in a species before it is considered
    not cleared for use in the training data
    :return: None
    """

    local_leaf_data_dict = leaf_data_dict.copy()  # to prevent changing master leaf data dict

    # filter leaves by min_leaves
    keys_to_remove = []
    for species in local_leaf_data_dict.keys():
        if len(local_leaf_data_dict[species]) < min_leaves:
            keys_to_remove.append(species)

    for key in keys_to_remove:
        local_leaf_data_dict.pop(key)
    
    # #################################
    #
    #         RANDOM SAMPLING
    #
    ###################################

    if num_species is not 'MAX':
        # randomly choose num_species leaves from the leaf data dict to train and run on
        chosen_species = {}
        for i in range(0, num_species):
            keys = [k for k in local_leaf_data_dict.keys()]
            j = choice(keys)
            print(j + ' chosen')

            chosen_species[j] = local_leaf_data_dict[j]

            # remove the key from the dict
            local_leaf_data_dict.pop(j)

        local_leaf_data_dict = chosen_species
    else:
        # do nothing if the number of species is set to max
        pass

    # to train and run the ANN on the same dataset, some leaves will have to be removed from the dataset first
    # move all leaves to the training data, as most leaves will reside there first
    train_leaves = {species: local_leaf_data_dict[species].copy() for species in local_leaf_data_dict.keys()}

    # create the run leaves as an empty dict as to add a few leaves of each species (from training leaves)
    run_leaves = {species: [] for species in local_leaf_data_dict.keys()}

    # remove leaves until 3 are removed
    for k in train_leaves.keys():
        for i in range(0, NUM_RUN_LEAVES):
            # take a random choice from the training leaves
            j = choice(train_leaves[k])

            # remove the leaf from the training dict and add it to the run leaves
            train_leaves[k].remove(j)
            run_leaves[k].append(j)

    # save the sampling
    save_leaf_samples(local_leaf_data_dict, train_leaves, run_leaves)
    
    # #################################
    #
    #        END RANDOM SAMPLING
    #
    ###################################

    # get a str list of all species possible
    name_list = [species for species in train_leaves.keys()]
    name_list.sort()

    # get the total number of entries
    total = 0
    for species in train_leaves.keys():
        total += len(train_leaves[species])

    # create the header for the file
    # calculate number of inputs

    # get a random leaf to count with
    sample_leaf = train_leaves[
        [k for k in train_leaves.keys()][0]
    ][0]

    inputs = {
        # simple measurements
        'p': sample_leaf['p'],
        'length': sample_leaf['length'],
        'area': sample_leaf['area'],
        'sf_v_mean': sample_leaf['sf_v_mean'],
        'sf_v_median': sample_leaf['sf_v_median'],

        # vein measurements  # vein measurements  
        'vein_angle_above': sample_leaf['vein_angle_above'],
        'vein_angle_below': sample_leaf['vein_angle_below'],
        'vein_length': sample_leaf['vein_length'],
    }

    # now add contours as inputs
    contour_size = sample_leaf['contour_size']
    # contour_pos = sample_leaf['contour_pos']
    contour_angle = sample_leaf['contour_angle']

    # now count
    num_inputs = len(inputs) + len(contour_size) + len(contour_angle)

    # one output for every species
    num_outputs = len([k for k in train_leaves.keys()])

    # CREATE THE HEADER
    header = [total, num_inputs, num_outputs]
    header = [str(i) for i in header]

    # save the header for main3,py, since the number of inputs and outputs are needed to train the ANN
    with open(DEFAULT_PICKLE_FILE, 'wb') as f:
        pickle.dump(header, f, pickle.HIGHEST_PROTOCOL)
        f.close()

    # create data that will be written to disk
    data_lines_list = [header]

    # now build the data lines
    for species in train_leaves.keys():

        leaf_list = train_leaves[species]

        # create output line now since it is the same for an entire species

        # get index of species name in sorted list
        index = name_list.index(species)
        # noinspection PyUnusedLocal
        out_line = [-1.0 for name in name_list]
        out_line[index] = 1.0

        for leaf in leaf_list:

            # ############## INPUT LINE #################

            #
            # order of data in training file 
            #
            # perimeter 
            # length 
            # area 
            # surface variability mean 
            # surface variability median 
            # vein angles above 
            # vein angles below
            # vein length
            # contour angles
            # contour positions
            # contour sizes

            # create line
            line = [
                leaf['p'],
                leaf['length'],
                leaf['area'],
                leaf['sf_v_mean'],
                leaf['sf_v_median'],
                leaf['vein_angle_above'],
                leaf['vein_angle_below'],
                leaf['vein_length']
            ]

            # extend line with contour information
            line.extend(leaf['contour_angle'].values())
            # line.extend(leaf['contour_pos'])
            line.extend(leaf['contour_size'].values())

            # append input AND output line
            data_lines_list.append(line)
            # noinspection PyTypeChecker
            data_lines_list.append(out_line)

    # save data to file
    with open(fname, mode='w') as f:
        for line in data_lines_list:
            # convert list to str in buffer
            buffer = ''
            for obj in line:
                buffer = '{0}{1} '.format(buffer, str(obj))
            buffer = buffer.strip(' ')
            buffer += '\n'

            # save to file
            f.write(buffer)
        f.write('\n')
        f.close()

    # save the samples of leaves to NPZ file
    np.savez_compressed(DEFAULT_SAMPLE_SAVE_FILE,
                        all=local_leaf_data_dict,
                        train=train_leaves,
                        run=run_leaves
                        )

    return local_leaf_data_dict, train_leaves, run_leaves


def save_leaf_samples(all_leaves, training_leaves, run_leaves, f_path=DEFAULT_SAMPLE_SAVE_FILE):
    """
    12/13/16

    This function takes the dictionaries returned by save_train() and saves those dictionaries. This is so that other
    programs can access the save data after the training data has been created. Since the sampling has to be done
    every time the training data is created, it makes sense to include it with these functions.

    :param f_path: path to save to
    :param all_leaves: dict of all leaves (training+run)
    :param training_leaves: dict of leaves in the training data
    :param run_leaves: dict of leaves that are to be run
    :return: None
    """
    np.savez_compressed(f_path,
                        all=all_leaves,
                        train=training_leaves,
                        run=run_leaves
                        )

    return None


def load_leaf_samples(f_path=None):
    """
    12/13/16

    This function takes the dictionaries saved by save_train() and loads those dictionaries. This is so that other
    programs can access the save data after the training data has been created. Since the sampling has to be done
    every time the training data is created, it makes sense to include it with these functions.

    :param f_path:
    :return: 3 dictionaries
    """
    if f_path is None:
        f_path = DEFAULT_SAMPLE_SAVE_FILE + '.npz'
    with np.load(f_path) as f:
        all_leaves = f['all'].tolist()
        train_leaves = f['train'].tolist()
        run_leaves = f['run'].tolist()
        f.close()

    return all_leaves, train_leaves, run_leaves


# run this if main
if __name__ == '__main__':
    # convert xml to training data
    convert(DEFAULT_XML, DEFAULT_TRAINING_FILE)
